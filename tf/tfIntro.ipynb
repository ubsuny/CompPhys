{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tensorflow as a numpy alternative\n",
    "[see TF numpy](https://www.tensorflow.org/guide/tf_numpy)\n",
    "\n",
    "TensorFlow implements a subset of the NumPy API, available as tf.experimental.numpy. This allows running NumPy code, accelerated by TensorFlow, while also allowing access to all of TensorFlow's APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import timeit\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # disbale CUDA warnings\n",
    "\n",
    "print(\"Using TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow NumPy ND array\n",
    "\n",
    "An instance of `tf.experimental.numpy.ndarray`, called **ND Array**, represents a multidimensional dense array of a given `dtype` placed on a certain device. It is an alias to `tf.Tensor`. Check out the ND array class for useful methods like `ndarray.T`, `ndarray.reshape`, `ndarray.ravel` and others.\n",
    "\n",
    "First create an ND array object, and then invoke different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ND array and check out different attributes.\n",
    "ones = tnp.ones([5, 3], dtype=tnp.float32)\n",
    "print(\"Created ND array with shape = %s, rank = %s, \"\n",
    "      \"dtype = %s on device = %s\\n\" % (\n",
    "          ones.shape, ones.ndim, ones.dtype, ones.device))\n",
    "\n",
    "# `ndarray` is just an alias to `tf.Tensor`.\n",
    "print(\"Is `ones` an instance of tf.Tensor: %s\\n\" % isinstance(ones, tf.Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type promotion\n",
    "> One special case of implicit type conversion is type promotion, where an object is automatically converted into another data type representing a superset of the original type\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Type_conversion#Type_promotion)\n",
    "\n",
    "Tensorflow has no automatic type promotion,\n",
    "\n",
    "numpy does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.promote_types(np.float32, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.promote_types(np.int32, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.promote_types(np.int32, np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.promote_types(np.complex64, np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/stable/reference/arrays.dtypes.html#arrays-dtypes\n",
    "np.promote_types(np.int8, np.datetime64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow we have to set the desired behavior:\n",
    "\n",
    "There are 4 options for type promotion in TensorFlow.\n",
    "\n",
    "- By default, TensorFlow raises errors instead of promoting types for mixed type operations.\n",
    "- Running `tf.numpy.experimental_enable_numpy_behavior()` switches TensorFlow to use `NumPy` type promotion rules (described below).\n",
    "- After TensorFlow 2.15, there are two new options (refer to [TF NumPy Type Promotion](tf_numpy_type_promotion.ipynb) for details):\n",
    "  - `tf.numpy.experimental_enable_numpy_behavior(dtype_conversion_mode=\"all\")` uses Jax type promotion rules.\n",
    "  - `tf.numpy.experimental_enable_numpy_behavior(dtype_conversion_mode=\"safe\")` uses Jax type promotion rules, but disallows certain unsafe promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp.experimental_enable_numpy_behavior(dtype_conversion_mode=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type promotion for operations\")\n",
    "values = [tnp.asarray(1, dtype=d) for d in\n",
    "          (tnp.int32, tnp.int64, tnp.float32, tnp.float64)]\n",
    "for i, v1 in enumerate(values):\n",
    "  for v2 in values[i + 1:]:\n",
    "    print(\"%s + %s => %s\" %\n",
    "          (v1.dtype.name, v2.dtype.name, (v1 + v2).dtype.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp.experimental_enable_numpy_behavior(dtype_conversion_mode=\"safe\")\n",
    "print(\"Type promotion for operations\")\n",
    "values = [tnp.asarray(1, dtype=d) for d in\n",
    "          (tnp.int32, tnp.int64, tnp.float32, tnp.float64)]\n",
    "for i, v1 in enumerate(values):\n",
    "  for v2 in values[i + 1:]:\n",
    "    print(\"%s + %s => %s\" %\n",
    "          (v1.dtype.name, v2.dtype.name, (v1 + v2).dtype.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type inference during array creation\")\n",
    "print(\"tnp.asarray(1).dtype == tnp.%s\" % tnp.asarray(1).dtype.name)\n",
    "print(\"tnp.asarray(1.).dtype == tnp.%s\\n\" % tnp.asarray(1.).dtype.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting literals to ND array, NumPy prefers wide types like `tnp.int64` and `tnp.float64`. In contrast, `tf.convert_to_tensor` prefers `tf.int32` and `tf.float32` types for converting constants to `tf.Tensor`. TensorFlow NumPy APIs adhere to the NumPy behavior for integers. As for floats, the `prefer_float32` argument of `experimental_enable_numpy_behavior` lets you control whether to prefer `tf.float32` over `tf.float64` (default to `False`). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp.experimental_enable_numpy_behavior(prefer_float32=True)\n",
    "print(\"When prefer_float32 is True:\")\n",
    "print(\"tnp.asarray(1.).dtype == tnp.%s\" % tnp.asarray(1.).dtype.name)\n",
    "print(\"tnp.add(1., 2.).dtype == tnp.%s\" % tnp.add(1., 2.).dtype.name)\n",
    "\n",
    "tnp.experimental_enable_numpy_behavior(prefer_float32=False)\n",
    "print(\"When prefer_float32 is False:\")\n",
    "print(\"tnp.asarray(1.).dtype == tnp.%s\" % tnp.asarray(1.).dtype.name)\n",
    "print(\"tnp.add(1., 2.).dtype == tnp.%s\" % tnp.add(1., 2.).dtype.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    ">The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes.\n",
    "\n",
    "Similar to TensorFlow, NumPy defines rich semantics for \"broadcasting\" values.\n",
    "Check out the [NumPy broadcasting guide](https://numpy.org/doc/1.16/user/basics.broadcasting.html) for more information and compare this with [TensorFlow broadcasting semantics](https://www.tensorflow.org/guide/tensor#broadcasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([2.0, 2.0, 2.0])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = 2.0\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[ 0.0,  0.0,  0.0],\n",
    "              [10.0, 10.0, 10.0],\n",
    "              [20.0, 20.0, 20.0],\n",
    "              [30.0, 30.0, 30.0]])\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones([2, 3])\n",
    "y = np.ones([3])\n",
    "z = np.ones([1, 2, 1])\n",
    "print(\"Broadcasting shapes %s, %s and %s gives shape %s\" % (\n",
    "    x.shape, y.shape, z.shape, (x + y + z).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tnp.array([[ 0.0,  0.0,  0.0],\n",
    "              [10.0, 10.0, 10.0],\n",
    "              [20.0, 20.0, 20.0],\n",
    "              [30.0, 30.0, 30.0]])\n",
    "b = tnp.array([1.0, 2.0, 3.0])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tnp.ones([2, 3])\n",
    "y = tnp.ones([3])\n",
    "z = tnp.ones([1, 2, 1])\n",
    "print(\"Broadcasting shapes %s, %s and %s gives shape %s\" % (\n",
    "    x.shape, y.shape, z.shape, (x + y + z).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "NumPy defines very sophisticated indexing rules. See the [NumPy Indexing guide](https://numpy.org/doc/1.16/reference/arrays.indexing.html). Note the use of ND arrays as indices below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tnp.arange(24).reshape(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic indexing\")\n",
    "print(x[1, tnp.newaxis, 1:3, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Boolean indexing\")\n",
    "print(x[:, (True, False, True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Advanced indexing\")\n",
    "print(x[1, (0, 0, 1), tnp.asarray([0, 1, 1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy interoperability\n",
    "\n",
    "TensorFlow ND arrays can interoperate with NumPy functions. These objects implement the `__array__` interface. NumPy uses this interface to convert function arguments to `np.ndarray` values before processing them.\n",
    "\n",
    "Similarly, TensorFlow NumPy functions can accept inputs of different types including `np.ndarray`. These inputs are converted to an ND array by calling `ndarray.asarray` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ND array passed into NumPy function.\n",
    "np_sum = np.sum(tnp.ones([2, 3]))\n",
    "print(\"sum = %s. Class: %s\" % (float(np_sum), np_sum.__class__))\n",
    "\n",
    "# `np.ndarray` passed into TensorFlow NumPy function.\n",
    "tnp_sum = tnp.sum(np.ones([2, 3]))\n",
    "print(\"sum = %s. Class: %s\" % (float(tnp_sum), tnp_sum.__class__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is easy to plot ND arrays, given the __array__ interface.\n",
    "labels = 15 + 2 * tnp.random.randn(1, 1000)\n",
    "_ = plt.hist(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, inputs, number=30, force_gpu_sync=False):\n",
    "  \"\"\"Utility to benchmark `f` on each value in `inputs`.\"\"\"\n",
    "  times = []\n",
    "  for inp in inputs:\n",
    "    def _g():\n",
    "      if force_gpu_sync:\n",
    "        one = tnp.asarray(1)\n",
    "      f(inp)\n",
    "      if force_gpu_sync:\n",
    "        with tf.device(\"CPU:0\"):\n",
    "          tnp.copy(one)  # Force a sync for GPU case\n",
    "\n",
    "    _g()  # warmup\n",
    "    t = timeit.timeit(_g, number=number)\n",
    "    times.append(t * 1000. / number)\n",
    "  return times\n",
    "\n",
    "\n",
    "def plot(np_times, tnp_times, compiled_tnp_times, has_gpu, tnp_times_gpu):\n",
    "  \"\"\"Plot the different runtimes.\"\"\"\n",
    "  plt.xlabel(\"size\")\n",
    "  plt.ylabel(\"time (ms)\")\n",
    "  plt.title(\"Sigmoid benchmark: TF NumPy vs NumPy\")\n",
    "  plt.plot(sizes, np_times, label=\"NumPy\")\n",
    "  plt.plot(sizes, tnp_times, label=\"TF NumPy (CPU)\")\n",
    "  plt.plot(sizes, compiled_tnp_times, label=\"Compiled TF NumPy (CPU)\")\n",
    "  if has_gpu:\n",
    "    plt.plot(sizes, tnp_times_gpu, label=\"TF NumPy (GPU)\")\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple implementation of `sigmoid`, and benchmark it using\n",
    "# NumPy and TensorFlow NumPy for different input sizes.\n",
    "\n",
    "def np_sigmoid(y):\n",
    "  return 1. / (1. + np.exp(-y))\n",
    "\n",
    "def tnp_sigmoid(y):\n",
    "  return 1. / (1. + tnp.exp(-y))\n",
    "\n",
    "@tf.function\n",
    "def compiled_tnp_sigmoid(y):\n",
    "  return tnp_sigmoid(y)\n",
    "\n",
    "sizes = (2 ** 0, 2 ** 5, 2 ** 10, 2 ** 15, 2 ** 20) #, 2 ** 25)\n",
    "np_inputs = [np.random.randn(size).astype(np.float32) for size in sizes]\n",
    "np_times = benchmark(np_sigmoid, np_inputs)\n",
    "\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "  tnp_inputs = [tnp.random.randn(size).astype(np.float32) for size in sizes]\n",
    "  tnp_times = benchmark(tnp_sigmoid, tnp_inputs)\n",
    "  compiled_tnp_times = benchmark(compiled_tnp_sigmoid, tnp_inputs)\n",
    "\n",
    "has_gpu = len(tf.config.list_logical_devices(\"GPU\"))\n",
    "if has_gpu:\n",
    "  with tf.device(\"/device:GPU:0\"):\n",
    "    tnp_inputs = [tnp.random.randn(size).astype(np.float32) for size in sizes]\n",
    "    tnp_times_gpu = benchmark(compiled_tnp_sigmoid, tnp_inputs, 100, True)\n",
    "else:\n",
    "  tnp_times_gpu = None\n",
    "plot(np_times, tnp_times, compiled_tnp_times, has_gpu, tnp_times_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
